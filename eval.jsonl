{"query": "Describe la arquitectura del encoder de un transformer", "expected_answer": "El encoder está compuesto por una pila de N = 6 capas idénticas. Cada capa tiene dos subcapas. La primera es un mecanismo de autoatención multi-cabeza, y la segunda es una red feed-forward totalmente conectada aplicada de manera posicional (posición por posición). Alrededor de cada una de las dos subcapas se emplea una conexión residual, seguida de normalización de capa.", "document": "1706.03762", "chunk_index": [4]}
{"query": "¿Qué puntuación BLEU alcanzó el Transformer en la tarea WMT 2014 inglés-alemán?", "expected_answer": "El Transformer alcanzó una puntuación BLEU de 28.4 en la tarea WMT 2014 inglés-alemán.", "document": "1706.03762", "chunk_index": [13]}
{"query": "¿Qué efecto tiene el tamaño del modelo en el rendimiento de BERT?", "expected_answer": "El tamaño del modelo BERT tiene un efecto positivo en su rendimiento: a medida que se aumentan el número de capas, unidades ocultas y cabezas de atención (es decir, se incrementa el tamaño del modelo), se observa una mejora estricta y consistente en la precisión en tareas de fine-tuning sobre distintos conjuntos de datos, incluso en aquellos con un número reducido de ejemplos de entrenamiento.", "document": "1810.04805", "chunk_index": [24]}
{"query": "¿Cómo se realizó el finetuning de BERT?", "expected_answer": "El fine-tuning de BERT se realiza añadiendo la capa de salida específica para la tarea (por ejemplo, clasificación con la representación del token [CLS] o etiquetado con las representaciones de cada token) y entrenando todo el modelo de manera end-to-end con los datos de la tarea. Simplemente se adaptan los inputs (texto único o pares de texto) y se ajustan todos los parámetros del modelo preentrenado.", "document": "1810.04805", "chunk_index": [12, 13]}
{"query": "¿Con qué datasets se preentrena XLNet?", "expected_answer": "XLNet se preentrena utilizando los siguientes datasets: Wikipedia, BooksCorpus, Giga5, ClueWeb 2012-B y Common Crawl.", "document": "1906.08237", "chunk_index": [15]}
{"query": "¿Cómo logra XLNet capturar contexto bidireccional sin usar máscaras como BERT?", "expected_answer": "XLNet logra capturar contexto bidireccional mediante Permutation Language Modeling: en vez de enmascarar tokens, entrena prediciendo cada posición bajo distintos órdenes de factorización de la secuencia. Así cada token ve información tanto de la izquierda como de la derecha, sin necesidad de usar el símbolo [MASK].", "document": "1906.08237", "chunk_index": [2]}
{"query": "¿Cuántos parámetros tiene ALBERT-large en comparación con BERT-large?", "expected_answer": "BERT-large tiene 334 millones de parámetros, mientras que ALBERT-large tiene solo 18 millones de parámetros.", "document": "1909.11942", "chunk_index": [15]}
{"query": "¿Qué problema tenía la tarea Next Sentence Prediction (NSP) en BERT?", "expected_answer": "El principal problema de NSP es su falta de dificultad como tarea. NSP combina la predicción de topic con la predicción de coherencia en una única tarea. Sin embargo, la predicción de topic es más fácil de aprender que la predicción de coherencia y se solapa notablemente con lo que se aprende usando la función de pérdida durante la tarea de Mask Language Modeling.", "document": "1909.11942", "chunk_index": [10]}
{"query": "¿Qué mejoras arquitectónicas se usaron en LLaMA respecto al Transformer original?", "expected_answer": "Las principales mejoras arquitectónicas presentadas en LLaMA con respecto al transformer original son: 1. Prenormalización: se normaliza la entrada de cada subcapa del transformer. 2. Función de activación SwiGLU: se sustituye ReLU por SwiGLU. Embeddings Rotatorios: se eliminan los embeddings posicionales y se cambian por embeddings rotarios.", "document": "2302.13971", "chunk_index": [6]}
{"query": "¿En qué tareas de commonsense reasoning se evaluó LLaMA?", "expected_answer": "LLaMA fue evaluado en ocho benchmarks estándar de commonsense reasoning: BoolQ, PIQA, SIQA, HellaSwag, WinoGrande, ARC (easy y challenge), y OpenBookQA.", "document": "2302.13971", "chunk_index": [12]}
