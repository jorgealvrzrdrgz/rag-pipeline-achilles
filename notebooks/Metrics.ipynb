{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66b09ef9-99fe-4313-a626-22974fcc66e2",
   "metadata": {},
   "source": [
    "En este notebook se realiza un análisis de los resultados obtenidos durante la inferencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "75e3a1a6-49ea-4906-aff4-245890f189f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc516cf-0a40-4a38-9d00-8d7ee3fb7c2f",
   "metadata": {},
   "source": [
    "Las métricas que tendremos en cuenta son:\n",
    "- Hit Rate\n",
    "- Recall\n",
    "- Mean Reciprocal Rank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6824550-2427-4797-a37d-e45128a12d23",
   "metadata": {},
   "source": [
    "Primero, definimos las funciones de las métricas que vamos a usar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "66cea725-f3d2-4d0c-8f6c-9234b79a06c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hit_rate_in_retrieval(preds: List[Dict], truths: List[Dict], k: int = 5) -> float:\n",
    "    aciertos = 0\n",
    "    total = len(truths)\n",
    "\n",
    "    for pred, truth in zip(preds, truths):\n",
    "        relevantes = {(truth[\"document\"], idx) for idx in truth[\"chunk_index\"]}\n",
    "        retrieved_topk = {\n",
    "            (c[\"document_name\"], c[\"chunk_index\"])\n",
    "            for c in pred.get(\"retrieved_chunks\", [])[:k]\n",
    "        }\n",
    "        if relevantes & retrieved_topk:\n",
    "            aciertos += 1\n",
    "\n",
    "    return aciertos / total if total > 0 else 0.0\n",
    "\n",
    "def hit_rate_in_response(preds: List[Dict], truths: List[Dict], k: int = 5) -> float:\n",
    "    aciertos = 0\n",
    "    total = len(truths)\n",
    "\n",
    "    for pred, truth in zip(preds, truths):\n",
    "        relevantes = {(truth[\"document\"], idx) for idx in truth[\"chunk_index\"]}\n",
    "        retrieved_topk = {\n",
    "            (c[\"document_name\"], c[\"chunk_index\"])\n",
    "            for c in pred.get(\"references\", [])[:k]\n",
    "        }\n",
    "        if relevantes & retrieved_topk:\n",
    "            aciertos += 1\n",
    "\n",
    "    return aciertos / total if total > 0 else 0.0\n",
    "\n",
    "def mrr(preds: List[Dict], truths: List[Dict], k: int =5) -> float:\n",
    "    rr_sum = 0.0\n",
    "    total = len(truths)\n",
    "\n",
    "    for pred, truth in zip(preds, truths):\n",
    "        relevantes = {(truth[\"document\"], idx) for idx in truth[\"chunk_index\"]}\n",
    "        retrieved = pred.get(\"retrieved_chunks\", [])[:k]\n",
    "\n",
    "        rank = None\n",
    "        for i, c in enumerate(retrieved, start=1):  # rank empieza en 1\n",
    "            if (c[\"document_name\"], c[\"chunk_index\"]) in relevantes:\n",
    "                rank = i\n",
    "                break\n",
    "\n",
    "        if rank is not None:\n",
    "            rr_sum += 1.0 / rank\n",
    "        # si no hay match, contribuye con 0\n",
    "\n",
    "    return rr_sum / total if total > 0 else 0.0\n",
    "\n",
    "def recall_at_k(\n",
    "    preds: List[Dict],\n",
    "    truths: List[Dict],\n",
    "    k: int = 10,\n",
    "    average: Literal[\"macro\", \"micro\"] = \"macro\",\n",
    "    return_per_query: bool = False,\n",
    ") -> Tuple[float, List[float] | None]:\n",
    "    \n",
    "    per_query_recalls = []\n",
    "    global_hits = 0\n",
    "    global_relevants = 0\n",
    "\n",
    "    for pred, truth in zip(preds, truths):\n",
    "        relevantes = {(truth[\"document\"], idx) for idx in truth.get(\"chunk_index\", [])}\n",
    "        # top-k recuperados (como conjunto para evitar contar duplicados)\n",
    "        retrieved_topk = {\n",
    "            (c[\"document_name\"], c[\"chunk_index\"])\n",
    "            for c in pred.get(\"retrieved_chunks\", [])[:k]\n",
    "        }\n",
    "\n",
    "        hits = len(relevantes & retrieved_topk)\n",
    "        total_rel = len(relevantes)\n",
    "\n",
    "        # recall por query (si no hay relevantes, definimos 0.0 para evitar NaN)\n",
    "        rq = (hits / total_rel) if total_rel > 0 else 0.0\n",
    "        per_query_recalls.append(rq)\n",
    "\n",
    "        global_hits += hits\n",
    "        global_relevants += total_rel\n",
    "\n",
    "    if average == \"macro\":\n",
    "        score = sum(per_query_recalls) / len(per_query_recalls) if per_query_recalls else 0.0\n",
    "    elif average == \"micro\":\n",
    "        score = (global_hits / global_relevants) if global_relevants > 0 else 0.0\n",
    "    else:\n",
    "        raise ValueError(\"average debe ser 'macro' o 'micro'.\")\n",
    "\n",
    "    return (score, per_query_recalls if return_per_query else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebf27c1-4847-4cef-827c-81fda2125c57",
   "metadata": {},
   "source": [
    "Cargamos los datos de evaluación y los resultados (tanto con el reranker como sin él)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "271d81ab-b38f-41c1-946e-1b9bc512503c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with open(\"../eval.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "results = []\n",
    "with open(\"../results.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        results.append(json.loads(line))\n",
    "\n",
    "results_no_reranker = []\n",
    "with open(\"../results_no_reranker.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        results_no_reranker.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6af7f14-e146-4a02-8fac-0296f393faa0",
   "metadata": {},
   "source": [
    "## Resultados con reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f0a273d6-77fa-4696-99f4-252dc87ed83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval Hit Rate: 1.0\n",
      "Response Hit Rate: 1.0\n",
      "Retrieval Recall: 0.95\n",
      "MRR: 0.75\n"
     ]
    }
   ],
   "source": [
    "print(f\"Retrieval Hit Rate: {hit_rate_in_retrieval(results, data)}\")\n",
    "print(f\"Response Hit Rate: {hit_rate_in_response(results, data)}\")\n",
    "print(f\"Retrieval Recall: {recall_at_k(results, data)[0]}\")\n",
    "print(f\"MRR: {mrr(results, data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658c0a9b-a288-493e-8eb1-682df99fe84a",
   "metadata": {},
   "source": [
    "## Resultados sin reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0b2866bb-4680-4336-b613-0859f966afec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval Hit Rate: 0.9\n",
      "Response Hit Rate: 0.9\n",
      "Retrieval Recall: 0.9\n",
      "MRR: 0.7666666666666666\n"
     ]
    }
   ],
   "source": [
    "print(f\"Retrieval Hit Rate: {hit_rate_in_retrieval(results_no_reranker, data)}\")\n",
    "print(f\"Response Hit Rate: {hit_rate_in_response(results_no_reranker, data)}\")\n",
    "print(f\"Retrieval Recall: {recall_at_k(results_no_reranker, data)[0]}\")\n",
    "print(f\"MRR: {mrr(results_no_reranker, data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e06943-b0ea-4bcb-bcaf-c19d80f04451",
   "metadata": {},
   "source": [
    "Como podemos ver, con el reranker se obtiene mejores métricas que con sin él. Aunque el MRR es ligeramente mayor sin reranker, lo cual nos hace ver que rankea los chunks relevantes más arriba, en el resto de métricas es mejor, y dado que no estamos tan interesados en rankear arriba como en rankear los chunks relevantes @k, podemos concluir que el reranker mejora el pipeline del RAG."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e166c45-bcd2-42e3-81aa-89c5005d63df",
   "metadata": {},
   "source": [
    "**NOTA**: para hacer este estudio de forma más rigurosa, además de utilizar un dataset mucho mayor, se debería haber comparado los resultados usando las mismas queries. Dada la naturleza no determinista de los LLMs, cada inferencia puede dar una query de búsqueda diferente. De todas formas, en ambos casos se ha probado que el pipeline RAG aquí implementado funciona correctamente."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
