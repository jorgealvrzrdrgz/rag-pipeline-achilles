{"query": "Describe la arquitectura del encoder de un transformer", "search_query": "arquitectura del encoder de un transformer", "answer": "La arquitectura del encoder de un transformer está compuesta por una pila de N = 6 capas idénticas. Cada capa consta de dos subcapas principales:\n\n1. Una capa de self-attention multi-cabeza (multi-head self-attention).\n2. Una red totalmente conectada que aplica una función feed-forward de manera individual a cada posición.\n\nCada subcapa utiliza una conexión residual alrededor del bloque, seguida de una normalización por capas (layer normalization). Es decir, la salida de cada subcapa se obtiene como LayerNorm(x + Sublayer(x)), donde Sublayer(x) es la función implementada por la subcapa correspondiente.\n\nTodas las subcapas y las capas de embedding producen salidas con la misma dimensión, típicamente d_model = 512. En el self-attention del encoder, todas las consultas, claves y valores provienen de la salida de la capa anterior del encoder, permitiendo que cada posición atienda a todas las demás de la estructura interna de esa capa.\n\nAdicionalmente, cada capa contiene una red feed-forward compuesta por dos transformaciones lineales separadas por una activación ReLU, aplicadas de manera individual a cada posición de la secuencia. Los parámetros de estas transformaciones varían entre capas pero son los mismos para todas las posiciones dentro de una capa.\n\nFinalmente, se agregan embeddings posicionales a los vectores de entrada para proporcionar información sobre el orden de las posiciones en la secuencia de entrada, ya que la autoatención en sí misma es independiente de la posición [1706.03762::4][1706.03762::7].", "retrieved_chunks": [{"document_name": "1706.03762", "chunk_index": 4}, {"document_name": "2302.13971", "chunk_index": 6}, {"document_name": "1906.08237", "chunk_index": 11}, {"document_name": "1706.03762", "chunk_index": 7}, {"document_name": "1706.03762", "chunk_index": 1}], "references": [{"document_name": "1706.03762", "chunk_index": 4}, {"document_name": "1706.03762", "chunk_index": 7}]}
{"query": "¿Qué puntuación BLEU alcanzó el Transformer en la tarea WMT 2014 inglés-alemán?", "search_query": "puntuación BLEU Transformer WMT 2014 inglés-alemán", "answer": "El modelo Transformer alcanzó una puntuación BLEU de 28.4 en la tarea WMT 2014 inglés-alemán (usando la configuración \"Transformer (big)\"). Esta puntuación superó a todos los modelos previos, incluidos los ensamblados, y estableció un nuevo estado del arte en ese momento [1706.03762::0][1706.03762::13][1706.03762::14].", "retrieved_chunks": [{"document_name": "1706.03762", "chunk_index": 13}, {"document_name": "1706.03762", "chunk_index": 14}, {"document_name": "1706.03762", "chunk_index": 0}, {"document_name": "2302.13971", "chunk_index": 54}, {"document_name": "1810.04805", "chunk_index": 45}], "references": [{"document_name": "1706.03762", "chunk_index": 0}, {"document_name": "1706.03762", "chunk_index": 13}, {"document_name": "1706.03762", "chunk_index": 14}]}
{"query": "¿Qué efecto tiene el tamaño del modelo en el rendimiento de BERT?", "search_query": "efecto del tamaño del modelo en el rendimiento de BERT", "answer": "El tamaño del modelo tiene un efecto directo y positivo en el rendimiento de BERT: al aumentar el número de capas, unidades ocultas y cabezas de atención, se observa una mejora consistente en la precisión en tareas de fine-tuning, incluso en aquellas con pocos ejemplos de entrenamiento como MRPC. Los experimentos muestran que modelos más grandes (como BERTLARGE con 340M parámetros frente a BERTBASE con 110M) obtienen mejoras de precisión en todos los conjuntos de datos evaluados, tanto en tareas grandes como pequeñas, siempre que el modelo haya sido previamente preentrenado de manera suficiente. Esto confirma que escalar BERT a tamaños extremos mejora notoriamente el desempeño en tareas downstream, incluidas las de menor escala [1810.04805::24][1810.04805::25][1810.04805::16][1810.04805::26].", "retrieved_chunks": [{"document_name": "1810.04805", "chunk_index": 24}, {"document_name": "1810.04805", "chunk_index": 26}, {"document_name": "1909.11942", "chunk_index": 15}, {"document_name": "1810.04805", "chunk_index": 25}, {"document_name": "1810.04805", "chunk_index": 16}], "references": [{"document_name": "1810.04805", "chunk_index": 24}, {"document_name": "1810.04805", "chunk_index": 25}, {"document_name": "1810.04805", "chunk_index": 16}, {"document_name": "1810.04805", "chunk_index": 26}]}
{"query": "¿Cómo se realizó el finetuning de BERT?", "search_query": "¿Cómo se realizó el finetuning de BERT?", "answer": "El finetuning de BERT se realiza en dos pasos: primero, el modelo es preentrenado usando datos no etiquetados sobre tareas de preentrenamiento (enmascarado de palabras y predicción de la próxima oración). Luego, para el finetuning, BERT se inicializa con los parámetros preentrenados y todos esos parámetros son ajustados usando datos etiquetados del task específico (por ejemplo, clasificación de texto, preguntas-respuestas, etc.). Cada tarea descendente (downstream task) tiene su propio modelo finetuneado, aunque todos son inicializados con los mismos parámetros preentrenados. \n\nLa arquitectura de BERT utilizada en preentrenamiento y finetuning es prácticamente la misma, solamente se incorpora una capa de salida adicional específica para la tarea durante el finetuning. Por ejemplo, para una tarea de clasificación, se añade una capa linear encima de la representación del token [CLS]. Durante el finetuning, todos los parámetros de BERT, más la nueva capa de salida, son ajustados simultáneamente con el set de datos rápido-ligado a la tarea objetivo [1810.04805::6][1810.04805::7][1810.04805::42][1810.04805::43].", "retrieved_chunks": [{"document_name": "1810.04805", "chunk_index": 42}, {"document_name": "1810.04805", "chunk_index": 27}, {"document_name": "1810.04805", "chunk_index": 7}, {"document_name": "1810.04805", "chunk_index": 43}, {"document_name": "1810.04805", "chunk_index": 6}], "references": [{"document_name": "1810.04805", "chunk_index": 7}, {"document_name": "1810.04805", "chunk_index": 42}, {"document_name": "1810.04805", "chunk_index": 43}]}
{"query": "¿Con qué datasets se preentrena XLNet?", "search_query": "datasets usados para preentrenar XLNet", "answer": "XLNet se preentrena utilizando los siguientes datasets principales:\n\n- Wikipedia\n- BooksCorpus\n- Giga5\n- ClueWeb 2012-B\n- Common Crawl\n\nEn particular, para algunos experimentos (como la comparación directa con BERT), sólo se usan Wikipedia y BooksCorpus. Para el entrenamiento a gran escala de XLNet-Large, se emplean todos los corpus mencionados, filtrando agresivamente artículos cortos o de baja calidad en ClueWeb y Common Crawl. Tras la tokenización, el conjunto total de datos supera los 32.89 mil millones de subpalabras [1906.08237::15].", "retrieved_chunks": [{"document_name": "1906.08237", "chunk_index": 15}, {"document_name": "1906.08237", "chunk_index": 34}, {"document_name": "1906.08237", "chunk_index": 16}, {"document_name": "2302.13971", "chunk_index": 3}, {"document_name": "2302.13971", "chunk_index": 42}], "references": [{"document_name": "1906.08237", "chunk_index": 15}]}
{"query": "¿Cómo logra XLNet capturar contexto bidireccional sin usar máscaras como BERT?", "search_query": "¿Cómo permite XLNet capturar contexto bidireccional sin usar máscaras como BERT?", "answer": "XLNet logra capturar el contexto bidireccional sin recurrir al uso de máscaras como BERT gracias a su técnica de \"permutación del orden de factorización\". En lugar de predecir una palabra a partir de una secuencia fija de palabras anteriores (como en los modelos autoregresivos clásicos) o usar máscaras artificiales como en BERT, XLNet maximiza la probabilidad logarítmica esperada de una secuencia respecto a todas las permutaciones posibles del orden de las palabras. Así, el contexto para cada posición puede consistir en tokens tanto a la izquierda como a la derecha, permitiendo en expectativa que cada posición utilice información contextual de todo el texto.\n\nDe este modo, XLNet captura la bidireccionalidad del contexto de forma natural y, además, evita la discrepancia entre preentrenamiento y ajuste fino que ocurre en BERT por la presencia de máscaras artificiales que no existen durante el uso real del modelo. El modelo nunca ve datos \"corrompidos\" y tampoco asume independencia entre las palabras que predice, lo que le permite modelar dependencias de orden superior y largo alcance de forma eficaz [1906.08237::2].", "retrieved_chunks": [{"document_name": "1906.08237", "chunk_index": 2}, {"document_name": "1906.08237", "chunk_index": 37}, {"document_name": "1906.08237", "chunk_index": 23}, {"document_name": "1906.08237", "chunk_index": 38}, {"document_name": "1906.08237", "chunk_index": 16}], "references": [{"document_name": "1906.08237", "chunk_index": 2}]}
{"query": "¿Cuántos parámetros tiene ALBERT-large en comparación con BERT-large?", "search_query": "parámetros ALBERT-large comparado con BERT-large", "answer": "ALBERT-large tiene aproximadamente 18 millones de parámetros, mientras que BERT-large tiene unos 334 millones de parámetros. Por lo tanto, ALBERT-large tiene unas 18 veces menos parámetros que BERT-large, lo que supone una mejora significativa en eficiencia de parámetros [1909.11942::11][1909.11942::15].", "retrieved_chunks": [{"document_name": "1909.11942", "chunk_index": 15}, {"document_name": "1909.11942", "chunk_index": 12}, {"document_name": "1909.11942", "chunk_index": 14}, {"document_name": "1909.11942", "chunk_index": 50}, {"document_name": "1909.11942", "chunk_index": 11}], "references": [{"document_name": "1909.11942", "chunk_index": 11}, {"document_name": "1909.11942", "chunk_index": 15}]}
{"query": "¿Qué problema tenía la tarea Next Sentence Prediction (NSP) en BERT?", "search_query": "problema de la tarea Next Sentence Prediction (NSP) en BERT", "answer": "El problema principal de la tarea Next Sentence Prediction (NSP) en BERT es que, aunque originalmente fue diseñada para ayudar en tareas como inferencia y comprensión de relaciones entre pares de frases, estudios posteriores encontraron que su impacto en el rendimiento era inconsistente y, a veces, incluso negativo. En particular, investigaciones señalaron que la tarea de NSP era demasiado sencilla porque terminaba siendo más un problema de predicción de tema (topic prediction) que de coherencia (coherence prediction), y la predicción de tema es más fácil de aprender y se solapa mucho con lo que ya aprende el modelo mediante el objetivo MLM (Masked Language Model). Por eso, modelos posteriores como ALBERT reemplazaron NSP por tareas que realmente modelan la coherencia entre frases, como la Sentence Order Prediction (SOP) [1909.11942::10].\n\nEn resumen, el principal problema de NSP en BERT es que no fomenta efectivamente el aprendizaje sobre relaciones de coherencia entre oraciones, haciéndolo menos útil para los objetivos de comprensión y modelado inter-oracional que se buscaban originalmente. Por eso, muchas variantes modernas ya no la utilizan o la han sustituido por tareas mejor diseñadas [1909.11942::10].", "retrieved_chunks": [{"document_name": "1810.04805", "chunk_index": 22}, {"document_name": "1810.04805", "chunk_index": 11}, {"document_name": "1909.11942", "chunk_index": 10}, {"document_name": "1810.04805", "chunk_index": 39}, {"document_name": "1810.04805", "chunk_index": 26}], "references": [{"document_name": "1909.11942", "chunk_index": 10}]}
{"query": "¿Qué mejoras arquitectónicas se usaron en LLaMA respecto al Transformer original?", "search_query": "mejoras arquitectónicas LLaMA respecto Transformer original", "answer": "Las principales mejoras arquitectónicas que usó LLaMA respecto al Transformer original son:\n\n- Pre-normalización: LLaMA normaliza la entrada de cada sub-capa del Transformer en vez de la salida, utilizando la función RMSNorm, lo que mejora la estabilidad del entrenamiento.\n- SwiGLU: Se reemplaza la función de activación ReLU por SwiGLU, que mejora el rendimiento general.\n- Rotary Embeddings (RoPE): Se eliminan las embeddings posicionales absolutas y se agregan embeddings posicionales rotatorios en cada capa.\n\nEstas mejoras están inspiradas en avances usados en modelos como GPT-3, PaLM y GPTNeo, y buscan mejorar tanto la eficiencia como la calidad del modelo respecto al Transformer clásico propuesto por Vaswani et al. (2017) [2302.13971::6].", "retrieved_chunks": [{"document_name": "2302.13971", "chunk_index": 54}, {"document_name": "2302.13971", "chunk_index": 51}, {"document_name": "2302.13971", "chunk_index": 6}, {"document_name": "2302.13971", "chunk_index": 2}, {"document_name": "2302.13971", "chunk_index": 17}], "references": [{"document_name": "2302.13971", "chunk_index": 6}]}
{"query": "¿En qué tareas de commonsense reasoning se evaluó LLaMA?", "search_query": "tareas de commonsense reasoning en las que se evaluó LLaMA", "answer": "LLaMA fue evaluado en ocho benchmarks estándar de commonsense reasoning: BoolQ, PIQA, SIQA, HellaSwag, WinoGrande, ARC (en sus variantes easy y challenge) y OpenBookQA. Estos benchmarks incluyen tareas de tipo Cloze, Winograd y preguntas de opción múltiple, y la evaluación se realizó en modo zero-shot [2302.13971::12][2302.13971::5].", "retrieved_chunks": [{"document_name": "2302.13971", "chunk_index": 12}, {"document_name": "2302.13971", "chunk_index": 16}, {"document_name": "2302.13971", "chunk_index": 14}, {"document_name": "2302.13971", "chunk_index": 10}, {"document_name": "2302.13971", "chunk_index": 54}], "references": [{"document_name": "2302.13971", "chunk_index": 12}]}
